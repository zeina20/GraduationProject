{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CNN implementation","metadata":{"id":"sWX_eEsQDvGU"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLThUTooASMr","outputId":"cf081925-1c41-4b90-f336-ef21492497ed","execution":{"iopub.status.busy":"2023-05-01T20:34:52.913424Z","iopub.execute_input":"2023-05-01T20:34:52.913983Z","iopub.status.idle":"2023-05-01T20:34:52.940299Z","shell.execute_reply.started":"2023-05-01T20:34:52.913944Z","shell.execute_reply":"2023-05-01T20:34:52.939428Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load Libraries","metadata":{"id":"-5C-UJ9CPOEi"}},{"cell_type":"markdown","source":"## 2. Load Dataset","metadata":{"id":"ts2PphD5DvGd"}},{"cell_type":"markdown","source":"## Extracting faces from train videos","metadata":{"id":"Q06Dmq6bn4SX"}},{"cell_type":"markdown","source":"## 3. Training Visualization ","metadata":{"id":"LOsuoq48DvGh"}},{"cell_type":"markdown","source":"## 4. Load Weights","metadata":{"id":"8TviGPXlDvGj"}},{"cell_type":"markdown","source":"## 5. Data Exploration","metadata":{"id":"PWpZJ0AgDvGk"}},{"cell_type":"code","source":"#plt.bar(range(2), [len(train_files_orig),len(train_files_fake)])","metadata":{"id":"-cpbLVZvDvGl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Image Sample","metadata":{"id":"hqjicuj9DvGm"}},{"cell_type":"markdown","source":"## 7. Pre-process the Data","metadata":{"id":"seWwuJqqDvGo"}},{"cell_type":"markdown","source":"### Tensor Creation","metadata":{"id":"h4yVJhClDvGo"}},{"cell_type":"markdown","source":"### Image Augmentation For The Fake Photos","metadata":{"id":"sDc9oNSDDvGq"}},{"cell_type":"markdown","source":"# Data generator","metadata":{"id":"MDb435uS6yIg"}},{"cell_type":"code","source":"# create and configure augmented image generator\nfrom keras.preprocessing.image import ImageDataGenerator\ntrain_gen = ImageDataGenerator(\n    rescale=1./255\n    #width_shift_range=0.2,  # randomly shift images horizontally (10% of total width)\n    #height_shift_range=0.2,  # randomly shift images vertically (10% of total height)\n    #horizontal_flip=True,\n    #vertical_flip = True,\n    #shear_range=0.15,\n    #zoom_range=0.2,\n) # randomly flip images horizontally\ntrain_generator = train_gen.flow_from_directory(\n        # This is the target directory\n        \"/kaggle/input/deep-fake-gp/train\",\n        # All images will be resized to 150x150\n        target_size=(224, 224),\n        batch_size=32,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary',\n        #color_mode=\"grayscale\"\n        )\nval_gen = ImageDataGenerator(rescale=1./255)\nval_generator = val_gen.flow_from_directory(\n        # This is the target directory\n        \"/kaggle/input/deep-fake-gp/val\",\n        # All images will be resized to 150x150\n        target_size=(224, 224),\n        batch_size=32,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary',\n        #color_mode=\"grayscale\"\n        )","metadata":{"id":"K_jxwdCP4NjY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd44bf4e-e0e4-46e6-d7d5-fbf04d18f845","execution":{"iopub.status.busy":"2023-05-01T20:35:02.033204Z","iopub.execute_input":"2023-05-01T20:35:02.034183Z","iopub.status.idle":"2023-05-01T20:35:09.431353Z","shell.execute_reply.started":"2023-05-01T20:35:02.034128Z","shell.execute_reply":"2023-05-01T20:35:09.430262Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 1615 images belonging to 2 classes.\nFound 409 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 8. Custom Architecture","metadata":{"id":"YecnnBsxDvGr"}},{"cell_type":"code","source":"# !pip uninstall tensorflow\n# !pip install tensorflow==1.4.0\n!pip install tensorflow\n\n# !pip uninstall keras\n# !pip install keras==2.0.8\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense\nfrom tensorflow.keras import Sequential\nfrom tensorflow import keras\n# from tensorflow.keras import layers\n# !pip install tensorflow.keras.layers.normalization\n!pip install keras-layer-normalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras_layer_normalization import LayerNormalization\n# from keras.layers.normalization import BatchNormalization\n\n# from LayerNormalization import BatchNormalization\n\"\"\"\n    Propsoed CNN architecture.\n\n\"\"\"\n\nmodel = Sequential()\n\n# Pamameters Initialization\ninput_shape = ( 224, 224, 3)\nactivation = 'relu'\npadding = 'same'\ndroprate = 0.1\nepsilon=0.001\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=input_shape))\nmodel.add(Conv2D(filters=16, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\n\n\nmodel.add(Conv2D(filters=32, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\nmodel.add(Dropout(droprate))\n\nmodel.add(Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\nmodel.add(Dropout(droprate))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\nmodel.add(Dropout(droprate))\n\nmodel.add(Conv2D(filters=256, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\nmodel.add(Dropout(droprate))\n\nmodel.add(Conv2D(filters=512, kernel_size=3, activation=activation, padding=padding))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(BatchNormalization(epsilon=epsilon))\nmodel.add(Dropout(droprate))\n\nmodel.add(GlobalAveragePooling2D())\n#model.add(Flatten())\n#model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu'))\n#model.add(Dropout(0.5))\n\n#model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))\n#model.add(Dropout(0.5))\n#model.add(Dropout(droprate))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary() # Summary of the architecture","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ir1pYfvfDvGs","outputId":"e374caef-3ba2-4b6d-ec2a-d8ba40ba223c","execution":{"iopub.status.busy":"2023-05-01T20:35:16.252778Z","iopub.execute_input":"2023-05-01T20:35:16.253503Z","iopub.status.idle":"2023-05-01T20:35:48.827332Z","shell.execute_reply.started":"2023-05-01T20:35:16.253464Z","shell.execute_reply":"2023-05-01T20:35:48.826380Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.11.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.1.21)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.29.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (15.0.6.1)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.2.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.35.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\nInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 21.12.2 requires cupy-cuda115, which is not installed.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\ntfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.19.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting keras-layer-normalization\n  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-layer-normalization) (1.21.6)\nBuilding wheels for collected packages: keras-layer-normalization\n  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=17509718dce2b9866d38a1ef388a8193f4f74caa74943f32a6a29b72a4f96b41\n  Stored in directory: /root/.cache/pip/wheels/41/f3/10/985c450e02ed9288fbc5145e90e4726ae95399eaa612a55ee2\nSuccessfully built keras-layer-normalization\nInstalling collected packages: keras-layer-normalization\nSuccessfully installed keras-layer-normalization-0.16.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n batch_normalization (BatchN  (None, 224, 224, 3)      12        \n ormalization)                                                   \n                                                                 \n conv2d (Conv2D)             (None, 224, 224, 16)      448       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n )                                                               \n                                                                 \n batch_normalization_1 (Batc  (None, 112, 112, 16)     64        \n hNormalization)                                                 \n                                                                 \n conv2d_1 (Conv2D)           (None, 112, 112, 32)      4640      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n 2D)                                                             \n                                                                 \n batch_normalization_2 (Batc  (None, 56, 56, 32)       128       \n hNormalization)                                                 \n                                                                 \n dropout (Dropout)           (None, 56, 56, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 56, 56, 64)        18496     \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 28, 28, 64)       0         \n 2D)                                                             \n                                                                 \n batch_normalization_3 (Batc  (None, 28, 28, 64)       256       \n hNormalization)                                                 \n                                                                 \n dropout_1 (Dropout)         (None, 28, 28, 64)        0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 28, 28, 128)       73856     \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n 2D)                                                             \n                                                                 \n batch_normalization_4 (Batc  (None, 14, 14, 128)      512       \n hNormalization)                                                 \n                                                                 \n dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 14, 14, 256)       295168    \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 7, 7, 256)        0         \n 2D)                                                             \n                                                                 \n batch_normalization_5 (Batc  (None, 7, 7, 256)        1024      \n hNormalization)                                                 \n                                                                 \n dropout_3 (Dropout)         (None, 7, 7, 256)         0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 7, 7, 512)         1180160   \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 3, 3, 512)        0         \n 2D)                                                             \n                                                                 \n batch_normalization_6 (Batc  (None, 3, 3, 512)        2048      \n hNormalization)                                                 \n                                                                 \n dropout_4 (Dropout)         (None, 3, 3, 512)         0         \n                                                                 \n global_average_pooling2d (G  (None, 512)              0         \n lobalAveragePooling2D)                                          \n                                                                 \n dense (Dense)               (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 1,577,325\nTrainable params: 1,575,303\nNon-trainable params: 2,022\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Compile","metadata":{"id":"Pv5PLYf-DvGu"}},{"cell_type":"code","source":"# Parameters Initialization\nfrom tensorflow.keras.optimizers import RMSprop , Adam\n# from tensorflow.python.keras.optimizers import rmsprop,SGD,Adam,Adadelta\n\n#opt = rmsprop(lr=0.0001, decay=1e-6)\n\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(), metrics=['accuracy'])","metadata":{"id":"OCzLuZ9xDvGu","execution":{"iopub.status.busy":"2023-05-01T20:36:06.367517Z","iopub.execute_input":"2023-05-01T20:36:06.367888Z","iopub.status.idle":"2023-05-01T20:36:06.383938Z","shell.execute_reply.started":"2023-05-01T20:36:06.367856Z","shell.execute_reply":"2023-05-01T20:36:06.382823Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{"id":"TocJbTX_DvGv"}},{"cell_type":"markdown","source":"## Train Model","metadata":{"id":"_cA7m6PCDvGw"}},{"cell_type":"markdown","source":"### Train Model on Original Data","metadata":{"id":"7ZgwH0VkDvGx"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping \n\nbatch_size = 10\nepochs = 20\n\ncheckpointer = ModelCheckpoint(filepath='weights.custom.best.hdf5',\n                               monitor='val_loss',verbose=1, save_best_only=True)\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\nhist = model.fit(train_generator,\n                #  initial_epoch=10,\n                    epochs=epochs, verbose=1, callbacks=[checkpointer,early_stopping],\n                    validation_data=val_generator,\n                     )\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLnUSItJDvGy","outputId":"044a6640-4d8e-44e2-9cd4-392d3326339d","execution":{"iopub.status.busy":"2023-05-01T20:36:11.261463Z","iopub.execute_input":"2023-05-01T20:36:11.262420Z","iopub.status.idle":"2023-05-01T20:43:24.368710Z","shell.execute_reply.started":"2023-05-01T20:36:11.262366Z","shell.execute_reply":"2023-05-01T20:43:24.367727Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2023-05-01 20:36:14.004067: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"51/51 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.5232\nEpoch 1: val_loss improved from inf to 0.74498, saving model to weights.custom.best.hdf5\n51/51 [==============================] - 40s 587ms/step - loss: 0.9483 - accuracy: 0.5232 - val_loss: 0.7450 - val_accuracy: 0.5306\nEpoch 2/20\n51/51 [==============================] - ETA: 0s - loss: 0.6997 - accuracy: 0.5585\nEpoch 2: val_loss improved from 0.74498 to 0.71929, saving model to weights.custom.best.hdf5\n51/51 [==============================] - 23s 459ms/step - loss: 0.6997 - accuracy: 0.5585 - val_loss: 0.7193 - val_accuracy: 0.5306\nEpoch 3/20\n51/51 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.5963\nEpoch 3: val_loss did not improve from 0.71929\n51/51 [==============================] - 23s 448ms/step - loss: 0.6741 - accuracy: 0.5963 - val_loss: 0.7665 - val_accuracy: 0.5306\nEpoch 4/20\n51/51 [==============================] - ETA: 0s - loss: 0.6589 - accuracy: 0.6037\nEpoch 4: val_loss did not improve from 0.71929\n51/51 [==============================] - 23s 443ms/step - loss: 0.6589 - accuracy: 0.6037 - val_loss: 0.7515 - val_accuracy: 0.5306\nEpoch 5/20\n51/51 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.6334\nEpoch 5: val_loss did not improve from 0.71929\n51/51 [==============================] - 23s 444ms/step - loss: 0.6390 - accuracy: 0.6334 - val_loss: 0.9483 - val_accuracy: 0.5306\nEpoch 6/20\n51/51 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.6440\nEpoch 6: val_loss did not improve from 0.71929\n51/51 [==============================] - 23s 453ms/step - loss: 0.6241 - accuracy: 0.6440 - val_loss: 0.8705 - val_accuracy: 0.4743\nEpoch 7/20\n51/51 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6582\nEpoch 7: val_loss improved from 0.71929 to 0.71655, saving model to weights.custom.best.hdf5\n51/51 [==============================] - 23s 458ms/step - loss: 0.6134 - accuracy: 0.6582 - val_loss: 0.7166 - val_accuracy: 0.5770\nEpoch 8/20\n51/51 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.7170\nEpoch 8: val_loss improved from 0.71655 to 0.70117, saving model to weights.custom.best.hdf5\n51/51 [==============================] - 23s 455ms/step - loss: 0.5691 - accuracy: 0.7170 - val_loss: 0.7012 - val_accuracy: 0.5721\nEpoch 9/20\n51/51 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7337\nEpoch 9: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 459ms/step - loss: 0.5380 - accuracy: 0.7337 - val_loss: 0.8022 - val_accuracy: 0.5403\nEpoch 10/20\n51/51 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7443\nEpoch 10: val_loss did not improve from 0.70117\n51/51 [==============================] - 24s 462ms/step - loss: 0.5251 - accuracy: 0.7443 - val_loss: 0.7143 - val_accuracy: 0.5917\nEpoch 11/20\n51/51 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.7833\nEpoch 11: val_loss did not improve from 0.70117\n51/51 [==============================] - 22s 439ms/step - loss: 0.4689 - accuracy: 0.7833 - val_loss: 0.7871 - val_accuracy: 0.5795\nEpoch 12/20\n51/51 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.7944\nEpoch 12: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 447ms/step - loss: 0.4524 - accuracy: 0.7944 - val_loss: 0.7698 - val_accuracy: 0.6137\nEpoch 13/20\n51/51 [==============================] - ETA: 0s - loss: 0.3980 - accuracy: 0.8310\nEpoch 13: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 445ms/step - loss: 0.3980 - accuracy: 0.8310 - val_loss: 0.8424 - val_accuracy: 0.6112\nEpoch 14/20\n51/51 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.8526\nEpoch 14: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 454ms/step - loss: 0.3596 - accuracy: 0.8526 - val_loss: 1.0342 - val_accuracy: 0.5575\nEpoch 15/20\n51/51 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.8941\nEpoch 15: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 450ms/step - loss: 0.2782 - accuracy: 0.8941 - val_loss: 0.8486 - val_accuracy: 0.5966\nEpoch 16/20\n51/51 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8873\nEpoch 16: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 452ms/step - loss: 0.2949 - accuracy: 0.8873 - val_loss: 0.8367 - val_accuracy: 0.6064\nEpoch 17/20\n51/51 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9257\nEpoch 17: val_loss did not improve from 0.70117\n51/51 [==============================] - 24s 464ms/step - loss: 0.2120 - accuracy: 0.9257 - val_loss: 0.9239 - val_accuracy: 0.6308\nEpoch 18/20\n51/51 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9406\nEpoch 18: val_loss did not improve from 0.70117\n51/51 [==============================] - 23s 456ms/step - loss: 0.1691 - accuracy: 0.9406 - val_loss: 1.1128 - val_accuracy: 0.6112\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{"id":"Ly8DeUix_nYf"}},{"cell_type":"markdown","source":"### Evaluate Model on Orgiginal Data","metadata":{"id":"VEUYAuYpDvGz"}},{"cell_type":"markdown","source":"### Model Evaluation on Augumented Data","metadata":{"id":"sfXb1fHDDvG1"}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/weights.custom.best.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:44:39.677123Z","iopub.execute_input":"2023-05-01T20:44:39.677543Z","iopub.status.idle":"2023-05-01T20:44:39.748271Z","shell.execute_reply.started":"2023-05-01T20:44:39.677506Z","shell.execute_reply":"2023-05-01T20:44:39.747241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# model = ...  # Get model (Sequential, Functional Model, or Model subclass)\nmodel.save('/kaggle/working/savedmodel.hdf5')","metadata":{"id":"qJLGKhMmJWU_","execution":{"iopub.status.busy":"2023-05-01T20:44:43.594592Z","iopub.execute_input":"2023-05-01T20:44:43.594997Z","iopub.status.idle":"2023-05-01T20:44:43.704637Z","shell.execute_reply.started":"2023-05-01T20:44:43.594963Z","shell.execute_reply":"2023-05-01T20:44:43.703568Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom tensorflow.keras.preprocessing import image\n\ndef pred(image_p):\n    img_path = image_p #'/kaggle/input/gray-yellow-r-lsp/Gray VS Yellow/test/Gray/WIN_20230418_06_44_46_Pro.jpg'\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224, 3))\n    # convert PIL.Image.Image type to 3D tensor with shape (96, 96, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 96, 96, 3) and return 4D tensor\n    x = np.expand_dims(x, axis=0)\n    x=x/255\n    return model.predict(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:15:02.573548Z","iopub.execute_input":"2023-05-01T21:15:02.574326Z","iopub.status.idle":"2023-05-01T21:15:02.580858Z","shell.execute_reply.started":"2023-05-01T21:15:02.574285Z","shell.execute_reply":"2023-05-01T21:15:02.579587Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\nimg = load_img('/kaggle/input/deep-fake-gp/test/training_real/real_00018.jpg', target_size=(224, 224))\nimg_array = img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array=img_array/255\nmodel.predict(img_array)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:14:09.270918Z","iopub.execute_input":"2023-05-01T21:14:09.271928Z","iopub.status.idle":"2023-05-01T21:14:09.349515Z","shell.execute_reply.started":"2023-05-01T21:14:09.271888Z","shell.execute_reply":"2023-05-01T21:14:09.348565Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 19ms/step\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"array([[0.6285674]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":" print(pred('/kaggle/input/deep-fake-gp/test/training_fake/mid_113_1100.jpg'))\n ","metadata":{}},{"cell_type":"code","source":"import os \nfor i in os.listdir(\"/kaggle/input/deep-fake-gp/test/\"):\n    print(\"==============\")\n    for ii in os.listdir(f'/kaggle/input/deep-fake-gp/test/{i}'):\n        print(f\"{ii} \",pred(f'/kaggle/input/deep-fake-gp/test/{i}/{ii}')[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:57:23.115741Z","iopub.execute_input":"2023-05-01T21:57:23.116202Z","iopub.status.idle":"2023-05-01T21:57:24.562797Z","shell.execute_reply.started":"2023-05-01T21:57:23.116157Z","shell.execute_reply":"2023-05-01T21:57:24.561699Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"==============\n1/1 [==============================] - 0s 49ms/step\nmid_397_1111.jpg  [0.25946933]\n1/1 [==============================] - 0s 38ms/step\nhard_232_0001.jpg  [0.6408263]\n1/1 [==============================] - 0s 33ms/step\nmid_360_1100.jpg  [0.36346555]\n1/1 [==============================] - 0s 28ms/step\nmid_183_1111.jpg  [0.67851704]\n1/1 [==============================] - 0s 27ms/step\nmid_113_1100.jpg  [0.65781164]\n1/1 [==============================] - 0s 28ms/step\nmid_307_1101.jpg  [0.49597174]\n1/1 [==============================] - 0s 27ms/step\nmid_40_1111.jpg  [0.40570378]\n1/1 [==============================] - 0s 28ms/step\nmid_479_1111.jpg  [0.63596225]\n==============\n1/1 [==============================] - 0s 26ms/step\nreal_01060.jpg  [0.78628725]\n1/1 [==============================] - 0s 27ms/step\nreal_00551.jpg  [0.6616218]\n1/1 [==============================] - 0s 26ms/step\nreal_00903.jpg  [0.59186]\n1/1 [==============================] - 0s 27ms/step\nreal_00675.jpg  [0.61157554]\n1/1 [==============================] - 0s 26ms/step\nreal_00225.jpg  [0.5650997]\n1/1 [==============================] - 0s 26ms/step\nreal_00722.jpg  [0.64117306]\n1/1 [==============================] - 0s 25ms/step\nreal_00982.jpg  [0.6497783]\n1/1 [==============================] - 0s 25ms/step\nreal_00018.jpg  [0.6285674]\n1/1 [==============================] - 0s 26ms/step\nreal_00815.jpg  [0.54828054]\n","output_type":"stream"}]},{"cell_type":"code","source":" print(pred('/kaggle/input/deep-fake-gp/test/training_fake/mid_113_1100.jpg'))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:08:00.626493Z","iopub.execute_input":"2023-05-01T21:08:00.626882Z","iopub.status.idle":"2023-05-01T21:08:00.707260Z","shell.execute_reply.started":"2023-05-01T21:08:00.626848Z","shell.execute_reply":"2023-05-01T21:08:00.706120Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 22ms/step\n[[1.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.predict(test_generator)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:07:54.284685Z","iopub.execute_input":"2023-05-01T21:07:54.285761Z","iopub.status.idle":"2023-05-01T21:07:54.718561Z","shell.execute_reply.started":"2023-05-01T21:07:54.285711Z","shell.execute_reply":"2023-05-01T21:07:54.717555Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 211ms/step\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"array([[0.78628725],\n       [0.6115755 ],\n       [0.25946945],\n       [0.40570366],\n       [0.63596225],\n       [0.66162163],\n       [0.49597162],\n       [0.36346534],\n       [0.5650996 ],\n       [0.6785169 ],\n       [0.6285673 ],\n       [0.6578115 ],\n       [0.6497782 ],\n       [0.6408262 ],\n       [0.6411729 ],\n       [0.5482806 ],\n       [0.59186   ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"test_gen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_gen.flow_from_directory(\n        # This is the target directory\n        \"/kaggle/input/deep-fake-gp/test\",\n        # All images will be resized to 150x150\n        target_size=(224, 224),\n        batch_size=32,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary',\n        #color_mode=\"grayscale\"\n        )","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:02:46.673189Z","iopub.execute_input":"2023-05-01T21:02:46.673957Z","iopub.status.idle":"2023-05-01T21:02:46.783425Z","shell.execute_reply.started":"2023-05-01T21:02:46.673916Z","shell.execute_reply":"2023-05-01T21:02:46.782454Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Found 17 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nimport shutil\nos.mkdir(\"testt\")\nfor i in os.listdir(\"/kaggle/input/deep-fake-gp/test/\"):\n    try:\n        \n        os.mkdir(f\"/kaggle/working/testt/{i}\")\n    except:\n        print(\"file exists\")\n    for ii in os.listdir(f'/kaggle/input/deep-fake-gp/test/{i}'):\n        shutil.copy(f'/kaggle/input/deep-fake-gp/test/{i}/{ii}',f'testt/{i}/{ii}')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:37:15.717582Z","iopub.execute_input":"2023-05-01T21:37:15.718845Z","iopub.status.idle":"2023-05-01T21:37:15.773682Z","shell.execute_reply.started":"2023-05-01T21:37:15.718788Z","shell.execute_reply":"2023-05-01T21:37:15.772606Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/testt.zip /kaggle/working/testt/","metadata":{"execution":{"iopub.status.busy":"2023-05-01T21:38:06.053617Z","iopub.execute_input":"2023-05-01T21:38:06.054388Z","iopub.status.idle":"2023-05-01T21:38:07.111281Z","shell.execute_reply.started":"2023-05-01T21:38:06.054348Z","shell.execute_reply":"2023-05-01T21:38:07.110053Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/testt/ (stored 0%)\n  adding: kaggle/working/testt/training_real/ (stored 0%)\n  adding: kaggle/working/testt/training_real/real_01060.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00018.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00982.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00903.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00225.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00551.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00722.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00675.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_real/real_00815.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/ (stored 0%)\n  adding: kaggle/working/testt/training_fake/mid_183_1111.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_360_1100.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_479_1111.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_307_1101.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_397_1111.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_113_1100.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/mid_40_1111.jpg (deflated 0%)\n  adding: kaggle/working/testt/training_fake/hard_232_0001.jpg (deflated 0%)\n","output_type":"stream"}]}]}